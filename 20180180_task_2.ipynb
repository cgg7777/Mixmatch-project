{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"20180180_task_2.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ps5P2lDYkc_a","colab_type":"text"},"source":["[Wide-ResNet MixMatch](#scrollTo=NMzmcxDzLgdv&line=3&uniqifier=1)\n","\n","[U_net MixMatch](#scrollTo=zPibROeofH1D&line=10&uniqifier=1)\n","\n","[ResNet18 MixMatch](#scrollTo=JYqDLKwkwbdS&line=9&uniqifier=1)\n","\n","[Wide-ResNet MixMatch + increasing epoch + increasing early stopping count + decreasing learning rate](#scrollTo=9gDzmZ03yrDs&line=7&uniqifier=1)\n","\n","\n","[Wide-ResNet MixMatch + increasing epoch + increasing early stopping count + decreasing learning rate + Weight Decay ](#scrollTo=EtxH5QRz4M4X&line=4&uniqifier=1)"]},{"cell_type":"code","metadata":{"id":"CicoqVnILgdi","colab_type":"code","outputId":"708153bf-16f8-42d3-e0f2-1b672ffb4938","executionInfo":{"status":"ok","timestamp":1576796610093,"user_tz":-540,"elapsed":38668,"user":{"displayName":"밈뭄멈뭄","photoUrl":"","userId":"02132713927266936024"}},"colab":{"base_uri":"https://localhost:8080/","height":283}},"source":["from __future__ import print_function\n","\n","!pip install progress\n","\n","import easydict\n","import os\n","import shutil\n","import time\n","import random\n","\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","\n","\n","import os\n","import sys\n","# 다른 py 파일을 import하기 위한 code 입니다. Google Drive 상에서 작동하게 하기 위한 코드로, /Colab Notebooks/20180180_task_2'가 제 환경에서의 개인적인 경로입니다.\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","sys.path.insert(0, '/content/gdrive/My Drive/Colab Notebooks/20180180_task_2')\n","\n","\n","# 아래 import 파일들은 모두 제공된 skeleton 입니다.\n","import models.wideresnet as models\n","import models.loss as loss\n","import dataset.cifar10 as dataset\n","from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting progress\n","  Downloading https://files.pythonhosted.org/packages/38/ef/2e887b3d2b248916fc2121889ce68af8a16aaddbe82f9ae6533c24ff0d2b/progress-1.5.tar.gz\n","Building wheels for collected packages: progress\n","  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for progress: filename=progress-1.5-cp36-none-any.whl size=8074 sha256=794784d3fff60fa1f9d76c7489bc6c718536655a7109e13876fded49f16b5146\n","  Stored in directory: /root/.cache/pip/wheels/6c/c8/80/32a294e3041f006c661838c05a411c7b7ffc60ff939d14e116\n","Successfully built progress\n","Installing collected packages: progress\n","Successfully installed progress-1.5\n","Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"diX2unHkLgdl","colab_type":"code","outputId":"96274be0-e9a5-4818-8789-52ebccdb3d38","executionInfo":{"status":"ok","timestamp":1576796610098,"user_tz":-540,"elapsed":10342,"user":{"displayName":"밈뭄멈뭄","photoUrl":"","userId":"02132713927266936024"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# 주어진 for_student.ipynb 의 기본 skeleton입니다.\n","\n","## This is argument.\n","args = easydict.EasyDict({\n","    \"epochs\": 16,   # number of total epochs to run\n","    \"batch_size\": 64, # train batchsize\n","    \"lr\": 0.002,       # initial learning rate\n","    \"resume\": '',     # path to latest checkpoint (default: none)\n","    \"gpu\": 0,\n","    \"T\" : 0.5,\n","    \"alpha\" : 0.75,\n","    \"n_labeled\": 250 , # Number of labeled data # please use 250 , 1000, 4000, 10000, 40000\n","    \"val_iteration\": 1024, # period of computing validation error\n","    \"out\": '', # Directory to output the result\n","    # You can add any configuration parameters for your own design!!\n","})\n","\n","print(args)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["{'epochs': 16, 'batch_size': 64, 'lr': 0.002, 'resume': '', 'gpu': 0, 'T': 0.5, 'alpha': 0.75, 'n_labeled': 250, 'val_iteration': 1024, 'out': ''}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"135Z1vbeLgdp","colab_type":"code","colab":{}},"source":["# 주어진 for_student.ipynb의 기본 skeleton 에서 일부를 수정하였습니다.\n","# interleave 함수를 제거하고,  mixmatch를 수행하기 위한 함수를 정의하였습니다.\n","# mixmatch 함수는 https://github.com/gan3sh500/mixmatch-pytorch/blob/master/notebook.ipynb 의 코드를 바탕으로\n","# 일부 수정하여 완성하였습니다.\n","\n","# mixmatch 함수를 기존 skeleton에 맞게 사용하기 위해 parameter로 unlabeled data를 합치치 않은 채로\n","# 2 set을 전달받고, 이를 torch.cat으로 합친 후 qb를 계산하였습니다.\n","\n","def sharpen(x, T = 0.5):   # Sharpening을 수행하는 함수\n","  pt = x**(1/T)\n","  target_u = pt/pt.sum(dim=1,keepdim=True)\n","  target_u = target_u.detach()\n","  return target_u\n","\n","def mixup(x1, p1, x2, p2, a = 0.75):  # mixup을 수행하는 함수\n","  Lambda = np.random.beta(a, a)\n","  Lambda = max(Lambda, 1 - Lambda)  \n","\n","  x = Lambda * x1 + (1 - Lambda) * x2\n","  y = Lambda * p1 + (1 - Lambda) * p2\n","  return x, y\n","\n","def mixmatch(xb, y, inputs_u, inputs_u2, model, K=2):    # K의 default값을 2로 지정하였음\n","    outputs_u = model(inputs_u)\n","    outputs_u2 = model(inputs_u2)\n","    ub = torch.cat([inputs_u, inputs_u2], dim = 0)\n","    p = (outputs_u + outputs_u2) / 2\n","    qb = sharpen(p)\n","    Ux = ub\n","    Uy = torch.cat([qb for _ in range(K)], dim=0)\n","\n","    indices = np.random.shuffle(np.arange(len(xb) + len(Ux)))\n","    Wx = torch.cat([Ux, xb], axis=0)[indices]\n","    Wy = torch.cat([Uy, y], axis=0)[indices]\n","\n","    X, p = mixup(xb, y, Wx[0][:len(xb)], Wy[0][:len(xb)])\n","\n","    U, q = mixup(ub, Uy, Wx[0][len(xb):], Wy[0][len(xb):])\n","\n","    return X, p, U, q\n","\n","\n","def train(labeled_trainloader, unlabeled_trainloader, model, optimizer, criterion, epoch, use_cuda=True):\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    losses_x = AverageMeter()\n","    losses_u = AverageMeter()\n","    ws = AverageMeter()\n","    end = time.time()\n","\n","    bar = Bar('Training', max=args.val_iteration)\n","    labeled_train_iter = iter(labeled_trainloader)\n","    unlabeled_train_iter = iter(unlabeled_trainloader)\n","\n","    model.train()\n","    for batch_idx in range(args.val_iteration):\n","        try:\n","            inputs_x, targets_x = labeled_train_iter.next()\n","        except:\n","            labeled_train_iter = iter(labeled_trainloader)\n","            inputs_x, targets_x = labeled_train_iter.next()\n","\n","        try:\n","            (inputs_u, inputs_u2), _ = unlabeled_train_iter.next()\n","        except:\n","            unlabeled_train_iter = iter(unlabeled_trainloader)\n","            (inputs_u, inputs_u2), _ = unlabeled_train_iter.next()\n","\n","        # measure data loading time\n","        data_time.update(time.time() - end)\n","\n","        batch_size = inputs_x.size(0)\n","\n","        # Transform label to one-hot\n","        targets_x = torch.zeros(batch_size, 10).scatter_(1, targets_x.view(-1,1), 1)\n","\n","        if use_cuda:\n","            inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n","            inputs_u = inputs_u.cuda()\n","            inputs_u2 = inputs_u2.cuda()\n","       \n","        # 아래 과정은 MixMatch가 일어나는 과정으로, 위에서 정의한 mixmatch, mixup, sharpen 함수를 사용합니다.\n","        with torch.no_grad():\n","            # compute guessed labels of unlabel samples\n","            outputs_u = model(inputs_u)\n","            outputs_u2 = model(inputs_u2)\n","\n","        unlabeled_target = torch.cat([outputs_u, outputs_u2], dim=0)\n","        all_inputs_u = torch.cat([inputs_u, inputs_u2])\n","        all_outputs_u = torch.cat([outputs_u, outputs_u2], dim = 0)\n","\n","\n","        mix_input, mix_target, mix_U, mix_p = mixmatch(inputs_x, targets_x, inputs_u, inputs_u2, model)\n","\n","        logits_x = model(mix_input)\n","        logits_u = model(mix_U)\n","\n","        Lx, Lu, w = criterion(logits_x, mix_target, logits_u, mix_p)\n","\n","\n","        loss = Lx + w * Lu\n","\n","        # record loss\n","        losses.update(loss.item(), inputs_x.size(0))\n","        losses_x.update(Lx.item(), inputs_x.size(0))\n","        losses_u.update(Lu.item(), inputs_x.size(0))\n","        ws.update(w, inputs_x.size(0))\n","\n","\n","        # compute gradient and do SGD step\n","        optimizer.zero_grad()\n","\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","\n","        # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","        # plot progress\n","\n","        if(batch_idx % 100 ==0):\n","          print('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | Loss: {loss:.4f} | Loss_x: {loss_x:.4f} | Loss_u: {loss_u:.4f}'.format(\n","                      batch=batch_idx + 1,\n","                      size=args.val_iteration,\n","                      data=data_time.avg,\n","                      bt=batch_time.avg,\n","                      total=bar.elapsed_td,\n","                      loss=losses.avg,\n","                      loss_x=losses_x.avg,\n","                      loss_u=losses_u.avg,\n","                      ))\n","          bar.next()\n","    bar.finish()\n","\n","    return (losses.avg, losses_x.avg, losses_u.avg,)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_YLkU2mLgdr","colab_type":"code","colab":{}},"source":["# 주어진 for_student.ipynb의 validation function에 해당하는 기본 skeleton 입니다.\n","\n","def validate(valloader, model, criterion, epoch, use_cuda, mode):\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    top1 = AverageMeter()\n","    top5 = AverageMeter()\n","\n","    # switch to evaluate mode\n","    model.eval()\n","\n","    end = time.time()\n","    bar = Bar(f'{mode}', max=len(valloader))\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(valloader):\n","            # measure data loading time\n","            data_time.update(time.time() - end)\n","\n","            if use_cuda:\n","                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n","\n","            # compute output\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            # measure accuracy and record loss\n","            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n","            losses.update(loss.item(), inputs.size(0))\n","            top1.update(prec1.item(), inputs.size(0))\n","            top5.update(prec5.item(), inputs.size(0))\n","\n","            # measure elapsed time\n","            batch_time.update(time.time() - end)\n","            end = time.time()\n","\n","            # plot progress\n","            if((batch_idx+1 ==len(valloader))):\n","              print('({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | Total: {total:} | Loss: {loss:.4f} | top1: {top1: .4f} | top5: {top5: .4f}'.format(\n","                          batch=batch_idx + 1,\n","                          size=len(valloader),\n","                          data=data_time.avg,\n","                          bt=batch_time.avg,\n","                          total=bar.elapsed_td,\n","                          loss=losses.avg,\n","                          top1=top1.avg,\n","                          top5=top5.avg,\n","                          ))\n","              bar.next()\n","        bar.finish()\n","    return (losses.avg, top1.avg)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"urURwVzPLgdt","colab_type":"code","outputId":"d8b7d947-5173-48a8-8581-263e1d4f0bd8","executionInfo":{"status":"ok","timestamp":1576796610101,"user_tz":-540,"elapsed":1156,"user":{"displayName":"밈뭄멈뭄","photoUrl":"","userId":"02132713927266936024"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# 주어진 for_student.ipynb의 기본 skeleton 입니다.\n","# Dataset을 로드해오는 기능을 수행합니다.\n","if not os.path.isdir(args.out):\n","    '''make dir if not exist'''\n","    try:\n","        os.makedirs(args.out)\n","    except:\n","        pass\n","\n","# Data\n","print(f'==> Preparing cifar10')\n","transform_train = transforms.Compose([\n","    dataset.RandomPadandCrop(32),\n","    dataset.RandomFlip(),\n","    dataset.ToTensor(),\n","])\n","\n","transform_val = transforms.Compose([\n","    dataset.ToTensor(),\n","])\n","\n","\n","\n","def CustomDataLoader(labeled_count):\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = dataset.get_cifar10('./data', labeled_count, transform_train=transform_train, transform_val=transform_val)\n","  return train_labeled_set, train_unlabeled_set, val_set, test_set\n","\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["==> Preparing cifar10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NMzmcxDzLgdv","colab_type":"code","colab":{}},"source":["# Wide_RestNet을 labeled data가 250, 1000, 4000, 10000개 있을 때의 훈련을 진행하여\n","# 각 case의 performance를 측정한다.\n","\n","def switch_state(x):   # for 문의 index를 통해 labeled data가 몇 개가 선택되어야 하는지를 결정하는 함수\n","    return {\n","        '0': 250,\n","        '1': 1000,\n","        '2': 4000,\n","        '3': 10000,\n","        '4': 30000\n","    }.get(x, 50000) #default\n","\n","\n","for i in range(0, 5):\n","  print(\"{}개의 labeled data를 이용하여 training 시작\\n\".format(switch_state(str(i))))\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = CustomDataLoader(int(switch_state(str(i))) )\n","\n","  labeled_trainloader = data.DataLoader(train_labeled_set, \n","                                        batch_size=args.batch_size, \n","                                        shuffle=True, \n","                                        num_workers=0, drop_last=True)\n","\n","  unlabeled_trainloader = data.DataLoader(train_unlabeled_set,\n","                                          batch_size=args.batch_size,\n","                                          shuffle=True,\n","                                          num_workers=0, drop_last=True)\n","\n","  val_loader = data.DataLoader(val_set,\n","                              batch_size=args.batch_size,\n","                              shuffle=False,\n","                              num_workers=0)\n","\n","  test_loader = data.DataLoader(test_set, \n","                                batch_size=args.batch_size,\n","                                shuffle=False, \n","                                num_workers=0)\n","\n","  ## run the model\n","  print(\"==> creating backbone network\")\n","\n","  model = models.WideResNet(num_classes=10)\n","  model = model.cuda()\n","\n","\n","  cudnn.benchmark = True\n","  print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n","\n","  print(\"==> defining loss function and optimizer\")\n","  train_criterion = loss.SemiLoss()\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=args.lr)\n","\n","  ## Training\n","  start_epoch = 0\n","\n","\n","  # early epoch 구현\n","  early_stopping_standard = 3\n","  early_stopping_count = 0\n","  temp_val_loss = 0\n","  best_epoch = 0\n","  for epoch in range(start_epoch, args.epochs):\n","      print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, args.epochs, args['lr']))\n","\n","      train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, train_criterion, epoch, use_cuda=True)\n","      _, train_acc = validate(labeled_trainloader, model, criterion, epoch, use_cuda=True, mode='Train Stats')\n","      val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda=True, mode='Valid Stats')\n","      test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda=True, mode='Test Stats ')\n","\n","      if epoch == 0:    # 첫 epoch에서\n","        temp_val_loss = val_loss    # 임시 validation loss 변수에 이번 epoch의 validation loss를 저장한다.\n","      else:\n","        if (temp_val_loss < val_loss):   # 임시 저장해놓은 validation loss보다 이번 epoch의 validation loss가 더 클 경우\n","          early_stopping_count = early_stopping_count+1    # early_stopping count를 1 증가시킨다.\n","        else:\n","          best_epoch = epoch+1                      # 만일 validation loss가 작아졌다면, best_epoch 기록을 남기고\n","          temp_val_loss = val_loss                  # 임시 validation loss를 초기화하고\n","          early_stopping_count = 0                  # early_stopping count를 초기화한다.\n","\n","      if(early_stopping_count == early_stopping_standard):           # early_stopping count가 제한 횟수와 같아질 경우\n","        print(\"최적 epoch : {}\\n\".format(best_epoch))               # print하고 학습을 종료한다.\n","        break;\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPibROeofH1D","colab_type":"code","colab":{}},"source":["# U_net model 정의\n","# https://github.com/usuyama/pytorch-unet/blob/master/pytorch_unet.py 에 기재된 U_net code를 일부\n","# 수정하여 사용하였습니다.\n","# CIFAR10 훈련에 사용하기 위해 output layer를 변경하였습니다.\n","\n","def double_conv(in_channels, out_channels):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True),\n","        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n","        nn.ReLU(inplace=True)\n","    )   \n","\n","\n","class UNet(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","                \n","        self.dconv_down1 = double_conv(3, 64)\n","        self.dconv_down2 = double_conv(64, 128)\n","        self.dconv_down3 = double_conv(128, 256)\n","        self.dconv_down4 = double_conv(256, 512)        \n","\n","        self.maxpool = nn.MaxPool2d(2)\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n","        \n","        self.dconv_up3 = double_conv(256 + 512, 256)\n","        self.dconv_up2 = double_conv(128 + 256, 128)\n","        self.dconv_up1 = double_conv(128 + 64, 64)\n","        \n","        self.conv_last = nn.Conv2d(64, 1, 1)\n","        self.fc = nn.Linear(32*32, 100)\n","        self.fc2 = nn.Linear (100, 10)\n","        \n","        \n","    def forward(self, x):\n","        conv1 = self.dconv_down1(x)\n","        x = self.maxpool(conv1)\n","\n","        conv2 = self.dconv_down2(x)\n","        x = self.maxpool(conv2)\n","        \n","        conv3 = self.dconv_down3(x)\n","        x = self.maxpool(conv3)   \n","        \n","        x = self.dconv_down4(x)\n","        \n","        x = self.upsample(x)        \n","        x = torch.cat([x, conv3], dim=1)\n","        \n","        x = self.dconv_up3(x)\n","        x = self.upsample(x)        \n","        x = torch.cat([x, conv2], dim=1)       \n","\n","        x = self.dconv_up2(x)\n","        x = self.upsample(x)        \n","        x = torch.cat([x, conv1], dim=1)   \n","        \n","        x = self.dconv_up1(x)\n","        \n","        x = self.conv_last(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        out = self.fc2(x)\n","        \n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2NhPQjVuGpf","colab_type":"code","colab":{}},"source":["\"\"\"\n","위와 동일하게 학습 알고리즘을 가지나,\n","model만 Unet으로 교체하여 진행하였다.\n","\"\"\"\n","torch.cuda.empty_cache()\n","\n","def switch_state(x):\n","    return {\n","        '0': 250,\n","        '1': 1000,\n","        '2': 4000,\n","        '3': 10000,\n","        '4': 30000\n","    }.get(x, 50000) #default\n","\n","for i in range(0, 5):\n","  print(\"{}개의 labeled data를 이용하여 training 시작\\n\".format(switch_state(str(i))))\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = CustomDataLoader(int(switch_state(str(i))) )\n","\n","  labeled_trainloader = data.DataLoader(train_labeled_set, \n","                                        batch_size=args.batch_size, \n","                                        shuffle=True, \n","                                        num_workers=0, drop_last=True)\n","\n","  unlabeled_trainloader = data.DataLoader(train_unlabeled_set,\n","                                          batch_size=args.batch_size,\n","                                          shuffle=True,\n","                                          num_workers=0, drop_last=True)\n","\n","  val_loader = data.DataLoader(val_set,\n","                              batch_size=args.batch_size,\n","                              shuffle=False,\n","                              num_workers=0)\n","\n","  test_loader = data.DataLoader(test_set, \n","                                batch_size=args.batch_size,\n","                                shuffle=False, \n","                                num_workers=0)\n","\n","  ## run the model\n","  print(\"==> creating backbone network\")\n","\n","  model = UNet()\n","  model = model.cuda()\n","\n","\n","  cudnn.benchmark = True\n","  print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n","\n","  print(\"==> defining loss function and optimizer\")\n","  train_criterion = loss.SemiLoss()\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","  ## Training\n","  start_epoch = 0\n","\n","\n","  # early epoch 구현\n","  early_stopping_standard = 3\n","  early_stopping_count = 0\n","  temp_val_loss = 0\n","  best_epoch = 0\n","  for epoch in range(start_epoch, args.epochs):\n","      print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, args.epochs, args['lr']))\n","\n","      train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, train_criterion, epoch, use_cuda=True)\n","      _, train_acc = validate(labeled_trainloader, model, criterion, epoch, use_cuda=True, mode='Train Stats')\n","      val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda=True, mode='Valid Stats')\n","      test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda=True, mode='Test Stats ')\n","\n","      if epoch == 0:\n","        temp_val_loss = val_loss\n","      else:\n","        if (temp_val_loss < val_loss):\n","          early_stopping_count = early_stopping_count+1\n","        else:\n","          best_epoch = epoch+1\n","          temp_val_loss = val_loss\n","          early_stopping_count = 0\n","\n","      if(early_stopping_count == early_stopping_standard):\n","        print(\"최적 epoch : {}\\n\".format(best_epoch))\n","        break;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYqDLKwkwbdS","colab_type":"code","colab":{}},"source":["\"\"\"\n","위와 동일하게 학습 알고리즘을 가지나,\n","model만 resnet18로 변경하여 학습하였다.\n","\"\"\"\n","import torchvision\n","torch.cuda.empty_cache()\n","\n","def switch_state(x):\n","    return {\n","        '0': 250,\n","        '1': 1000,\n","        '2': 4000,\n","        '3': 10000,\n","        '4': 30000\n","    }.get(x, 50000) #default\n","\n","for i in range(0, 5):\n","  print(\"{}개의 labeled data를 이용하여 training 시작\\n\".format(switch_state(str(i))))\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = CustomDataLoader(int(switch_state(str(i))) )\n","\n","  labeled_trainloader = data.DataLoader(train_labeled_set, \n","                                        batch_size=args.batch_size, \n","                                        shuffle=True, \n","                                        num_workers=0, drop_last=True)\n","\n","  unlabeled_trainloader = data.DataLoader(train_unlabeled_set,\n","                                          batch_size=args.batch_size,\n","                                          shuffle=True,\n","                                          num_workers=0, drop_last=True)\n","\n","  val_loader = data.DataLoader(val_set,\n","                              batch_size=args.batch_size,\n","                              shuffle=False,\n","                              num_workers=0)\n","\n","  test_loader = data.DataLoader(test_set, \n","                                batch_size=args.batch_size,\n","                                shuffle=False, \n","                                num_workers=0)\n","\n","  ## run the model\n","  print(\"==> creating backbone network\")\n","\n","  model = torchvision.models.resnet18(pretrained=False)\n","  model.fc = nn.Linear(512,10)\n","  model = model.cuda()\n","\n","\n","  cudnn.benchmark = True\n","  print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n","\n","  print(\"==> defining loss function and optimizer\")\n","  train_criterion = loss.SemiLoss()\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","  ## Training\n","  start_epoch = 0\n","\n","\n","  # early epoch 구현\n","  early_stopping_standard = 3\n","  early_stopping_count = 0\n","  temp_val_loss = 0\n","  best_epoch = 0\n","  for epoch in range(start_epoch, args.epochs):\n","      print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, args.epochs, args['lr']))\n","\n","      train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, train_criterion, epoch, use_cuda=True)\n","      _, train_acc = validate(labeled_trainloader, model, criterion, epoch, use_cuda=True, mode='Train Stats')\n","      val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda=True, mode='Valid Stats')\n","      test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda=True, mode='Test Stats ')\n","\n","      if epoch == 0:\n","        temp_val_loss = val_loss\n","      else:\n","        if (temp_val_loss < val_loss):\n","          early_stopping_count = early_stopping_count+1\n","        else:\n","          best_epoch = epoch+1\n","          temp_val_loss = val_loss\n","          early_stopping_count = 0\n","\n","      if(early_stopping_count == early_stopping_standard):\n","        print(\"최적 epoch : {}\\n\".format(best_epoch))\n","        break;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9gDzmZ03yrDs","colab_type":"code","colab":{}},"source":["# 위의 Wide-ResNet training과 동일하나, Epoch와 Early stopping 기준 count 를 증가시키고 performance를 체크하였다.\n","# 또한 learning rate를 감소시켰다.\n","torch.cuda.empty_cache()\n","def switch_state(x):\n","    return {\n","        '0': 250,\n","        '1': 1000,\n","        '2': 4000,\n","        '3': 10000,\n","        '4': 30000\n","    }.get(x, 50000) #default\n","\n","\n","for i in range(0, 5):\n","  print(\"{}개의 labeled data를 이용하여 training 시작\\n\".format(switch_state(str(i))))\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = CustomDataLoader(int(switch_state(str(i))) )\n","\n","  labeled_trainloader = data.DataLoader(train_labeled_set, \n","                                        batch_size=args.batch_size, \n","                                        shuffle=True, \n","                                        num_workers=0, drop_last=True)\n","\n","  unlabeled_trainloader = data.DataLoader(train_unlabeled_set,\n","                                          batch_size=args.batch_size,\n","                                          shuffle=True,\n","                                          num_workers=0, drop_last=True)\n","\n","  val_loader = data.DataLoader(val_set,\n","                              batch_size=args.batch_size,\n","                              shuffle=False,\n","                              num_workers=0)\n","\n","  test_loader = data.DataLoader(test_set, \n","                                batch_size=args.batch_size,\n","                                shuffle=False, \n","                                num_workers=0)\n","\n","  ## run the model\n","  print(\"==> creating backbone network\")\n","\n","  model = models.WideResNet(num_classes=10)\n","  model = model.cuda()\n","\n","\n","  cudnn.benchmark = True\n","  print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n","\n","  print(\"==> defining loss function and optimizer\")\n","  train_criterion = loss.SemiLoss()\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","  ## Training\n","  start_epoch = 0\n","\n","\n","  # early epoch 구현\n","  early_stopping_standard = 5\n","  early_stopping_count = 0\n","  temp_val_loss = 0\n","  best_epoch = 0\n","  for epoch in range(start_epoch, 50):\n","      print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, 50, 0.001))\n","\n","      train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, train_criterion, epoch, use_cuda=True)\n","      _, train_acc = validate(labeled_trainloader, model, criterion, epoch, use_cuda=True, mode='Train Stats')\n","      val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda=True, mode='Valid Stats')\n","      test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda=True, mode='Test Stats ')\n","\n","      if epoch == 0:\n","        temp_val_loss = val_loss\n","      else:\n","        if (temp_val_loss < val_loss):\n","          early_stopping_count = early_stopping_count+1\n","        else:\n","          best_epoch = epoch+1\n","          temp_val_loss = val_loss\n","          early_stopping_count = 0\n","\n","      if(early_stopping_count == early_stopping_standard):\n","        print(\"최적 epoch : {}\\n\".format(best_epoch))\n","        break;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtxH5QRz4M4X","colab_type":"code","colab":{}},"source":["# 위 Wide-ResNet에 3가지 변화를 준 모델에 L2 Legularization을 위해 weight decay를 추가하였다\n","# decay 상수값은 0.001로 설정하였다. \n","torch.cuda.empty_cache()\n","def switch_state(x):\n","    return {\n","        '0': 250,\n","        '1': 1000,\n","        '2': 4000,\n","        '3': 10000,\n","        '4': 30000\n","    }.get(x, 50000) #default\n","\n","\n","for i in range(0, 5):\n","  print(\"{}개의 labeled data를 이용하여 training 시작\\n\".format(switch_state(str(i))))\n","  train_labeled_set, train_unlabeled_set, val_set, test_set = CustomDataLoader(int(switch_state(str(i))) )\n","\n","  labeled_trainloader = data.DataLoader(train_labeled_set, \n","                                        batch_size=args.batch_size, \n","                                        shuffle=True, \n","                                        num_workers=0, drop_last=True)\n","\n","  unlabeled_trainloader = data.DataLoader(train_unlabeled_set,\n","                                          batch_size=args.batch_size,\n","                                          shuffle=True,\n","                                          num_workers=0, drop_last=True)\n","\n","  val_loader = data.DataLoader(val_set,\n","                              batch_size=args.batch_size,\n","                              shuffle=False,\n","                              num_workers=0)\n","\n","  test_loader = data.DataLoader(test_set, \n","                                batch_size=args.batch_size,\n","                                shuffle=False, \n","                                num_workers=0)\n","\n","  ## run the model\n","  print(\"==> creating backbone network\")\n","\n","  model = models.WideResNet(num_classes=10)\n","  model = model.cuda()\n","\n","\n","  cudnn.benchmark = True\n","  print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n","\n","  print(\"==> defining loss function and optimizer\")\n","  train_criterion = loss.SemiLoss()\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.001)  # weight decay 적용\n","\n","  ## Training\n","  start_epoch = 0\n","\n","\n","  # early epoch 구현\n","  early_stopping_standard = 5\n","  early_stopping_count = 0\n","  temp_val_loss = 0\n","  best_epoch = 0\n","  for epoch in range(start_epoch, 50):\n","      print('\\nEpoch: [%d | %d] LR: %f' % (epoch + 1, 50, 0.001))\n","\n","      train_loss, train_loss_x, train_loss_u = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, train_criterion, epoch, use_cuda=True)\n","      _, train_acc = validate(labeled_trainloader, model, criterion, epoch, use_cuda=True, mode='Train Stats')\n","      val_loss, val_acc = validate(val_loader, model, criterion, epoch, use_cuda=True, mode='Valid Stats')\n","      test_loss, test_acc = validate(test_loader, model, criterion, epoch, use_cuda=True, mode='Test Stats ')\n","\n","      if epoch == 0:\n","        temp_val_loss = val_loss\n","      else:\n","        if (temp_val_loss < val_loss):\n","          early_stopping_count = early_stopping_count+1\n","        else:\n","          best_epoch = epoch+1\n","          temp_val_loss = val_loss\n","          early_stopping_count = 0\n","\n","      if(early_stopping_count == early_stopping_standard):\n","        print(\"최적 epoch : {}\\n\".format(best_epoch))\n","        break;"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUgKtCJKoUUp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}